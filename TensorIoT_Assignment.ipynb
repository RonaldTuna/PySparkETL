{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorIoT_Assignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNCuVmMWRxAsl6X7kbv8RsR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RonaldTuna/TensorIoTAssignment/blob/main/TensorIoT_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEPS:**\n",
        "\n",
        "Download the data files from here - http://jmcauley.ucsd.edu/data/amazon/links.html\n",
        "\n",
        "Apache spark tools locally and necessary tools\n",
        "\n",
        "Download a review file with a million reviews\n",
        "\n",
        "Using Jupyter notebook create a program to read the million reviews and get the following\n",
        "\n",
        "transform date to MM-DD-YYYY format\n",
        "\n",
        "Save the data into a table (postgres/sql server)\n",
        "Save the output as a Parquet file\n",
        "\n",
        "\n",
        "Upload code to Github  and complete Readme.md which anyone can understand\n",
        "\n",
        "Send Github link to HR\n",
        "\n",
        "\n",
        "\n",
        "Skills Learning / Tools Used:\n",
        "\n",
        "    postgres\n",
        "    workbench/J\n",
        "    Jupyter notebook\n",
        "    Apache spark\n",
        "    Data frame transforms\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QFW_FbEwBco8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Java, Spark, Findspark and download a Postgresql driver\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.3.5.jar\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZnebPMZQCD7S",
        "outputId": "df404506-47a2-4b8d-87eb-250ee95c94eb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-12 19:48:16--  https://jdbc.postgresql.org/download/postgresql-42.3.5.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1041066 (1017K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.3.5.jar’\n",
            "\n",
            "postgresql-42.3.5.j 100%[===================>]   1017K  5.12MB/s    in 0.2s    \n",
            "\n",
            "2022-05-12 19:48:17 (5.12 MB/s) - ‘postgresql-42.3.5.jar’ saved [1041066/1041066]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  %pyspark_version 3.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PM-KAwZ9QPON",
        "outputId": "73c10041-2e4c-4995-b7a1-df860bc61db0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.1)\n",
            "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install postgresql server\n",
        "!sudo apt-get -y -qq update\n",
        "!sudo apt-get -y -qq install postgresql\n",
        "!sudo service postgresql start\n",
        "\n",
        "# Setup a password `postgres` for username `postgres`\n",
        "!sudo -u postgres psql -U postgres -c \"ALTER USER postgres PASSWORD 'postgres';\"\n",
        "\n",
        "# Setup a database with name `reviews` to be used\n",
        "!sudo -u postgres psql -U postgres -c 'DROP DATABASE IF EXISTS reviews;'\n",
        "!sudo -u postgres psql -U postgres -c 'CREATE DATABASE reviews;'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y97SOEe-T3m5",
        "outputId": "7f21585b-7345-4753-843f-70024b951dd6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 12.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package cron.\n",
            "(Reading database ... 155203 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cron_3.0pl1-128.1ubuntu1.2_amd64.deb ...\n",
            "Unpacking cron (3.0pl1-128.1ubuntu1.2) ...\n",
            "Selecting previously unselected package logrotate.\n",
            "Preparing to unpack .../01-logrotate_3.11.0-0.1ubuntu1_amd64.deb ...\n",
            "Unpacking logrotate (3.11.0-0.1ubuntu1) ...\n",
            "Selecting previously unselected package netbase.\n",
            "Preparing to unpack .../02-netbase_5.4_all.deb ...\n",
            "Unpacking netbase (5.4) ...\n",
            "Preparing to unpack .../03-libpq-dev_10.20-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libpq-dev (10.20-0ubuntu0.18.04.1) over (10.19-0ubuntu0.18.04.1) ...\n",
            "Preparing to unpack .../04-libpq5_10.20-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking libpq5:amd64 (10.20-0ubuntu0.18.04.1) over (10.19-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package postgresql-client-common.\n",
            "Preparing to unpack .../05-postgresql-client-common_190ubuntu0.1_all.deb ...\n",
            "Unpacking postgresql-client-common (190ubuntu0.1) ...\n",
            "Selecting previously unselected package postgresql-client-10.\n",
            "Preparing to unpack .../06-postgresql-client-10_10.20-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking postgresql-client-10 (10.20-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package ssl-cert.\n",
            "Preparing to unpack .../07-ssl-cert_1.0.39_all.deb ...\n",
            "Unpacking ssl-cert (1.0.39) ...\n",
            "Selecting previously unselected package postgresql-common.\n",
            "Preparing to unpack .../08-postgresql-common_190ubuntu0.1_all.deb ...\n",
            "Adding 'diversion of /usr/bin/pg_config to /usr/bin/pg_config.libpq-dev by postgresql-common'\n",
            "Unpacking postgresql-common (190ubuntu0.1) ...\n",
            "Selecting previously unselected package postgresql-10.\n",
            "Preparing to unpack .../09-postgresql-10_10.20-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking postgresql-10 (10.20-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package postgresql.\n",
            "Preparing to unpack .../10-postgresql_10+190ubuntu0.1_all.deb ...\n",
            "Unpacking postgresql (10+190ubuntu0.1) ...\n",
            "Selecting previously unselected package sysstat.\n",
            "Preparing to unpack .../11-sysstat_11.6.1-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking sysstat (11.6.1-1ubuntu0.1) ...\n",
            "Setting up sysstat (11.6.1-1ubuntu0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "\n",
            "Creating config file /etc/default/sysstat with new version\n",
            "update-alternatives: using /usr/bin/sar.sysstat to provide /usr/bin/sar (sar) in auto mode\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/sysstat.service → /lib/systemd/system/sysstat.service.\n",
            "Setting up ssl-cert (1.0.39) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Setting up libpq5:amd64 (10.20-0ubuntu0.18.04.1) ...\n",
            "Setting up cron (3.0pl1-128.1ubuntu1.2) ...\n",
            "Adding group `crontab' (GID 110) ...\n",
            "Done.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/cron.service → /lib/systemd/system/cron.service.\n",
            "update-rc.d: warning: start and stop actions are no longer supported; falling back to defaults\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up logrotate (3.11.0-0.1ubuntu1) ...\n",
            "Setting up netbase (5.4) ...\n",
            "Setting up libpq-dev (10.20-0ubuntu0.18.04.1) ...\n",
            "Setting up postgresql-client-common (190ubuntu0.1) ...\n",
            "Setting up postgresql-common (190ubuntu0.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Adding user postgres to group ssl-cert\n",
            "\n",
            "Creating config file /etc/postgresql-common/createcluster.conf with new version\n",
            "Building PostgreSQL dictionaries from installed myspell/hunspell packages...\n",
            "Removing obsolete dictionary files:\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/postgresql.service → /lib/systemd/system/postgresql.service.\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql-client-10 (10.20-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/share/postgresql/10/man/man1/psql.1.gz to provide /usr/share/man/man1/psql.1.gz (psql.1.gz) in auto mode\n",
            "Setting up postgresql-10 (10.20-0ubuntu0.18.04.1) ...\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76.)\n",
            "debconf: falling back to frontend: Readline\n",
            "Creating new PostgreSQL cluster 10/main ...\n",
            "/usr/lib/postgresql/10/bin/initdb -D /var/lib/postgresql/10/main --auth-local peer --auth-host md5\n",
            "The files belonging to this database system will be owned by user \"postgres\".\n",
            "This user must also own the server process.\n",
            "\n",
            "The database cluster will be initialized with locale \"en_US.UTF-8\".\n",
            "The default database encoding has accordingly been set to \"UTF8\".\n",
            "The default text search configuration will be set to \"english\".\n",
            "\n",
            "Data page checksums are disabled.\n",
            "\n",
            "fixing permissions on existing directory /var/lib/postgresql/10/main ... ok\n",
            "creating subdirectories ... ok\n",
            "selecting default max_connections ... 100\n",
            "selecting default shared_buffers ... 128MB\n",
            "selecting default timezone ... Etc/UTC\n",
            "selecting dynamic shared memory implementation ... posix\n",
            "creating configuration files ... ok\n",
            "running bootstrap script ... ok\n",
            "performing post-bootstrap initialization ... ok\n",
            "syncing data to disk ... ok\n",
            "\n",
            "Success. You can now start the database server using:\n",
            "\n",
            "    /usr/lib/postgresql/10/bin/pg_ctl -D /var/lib/postgresql/10/main -l logfile start\n",
            "\n",
            "Ver Cluster Port Status Owner    Data directory              Log file\n",
            "\u001b[31m10  main    5432 down   postgres /var/lib/postgresql/10/main /var/log/postgresql/postgresql-10-main.log\u001b[0m\n",
            "update-alternatives: using /usr/share/postgresql/10/man/man1/postmaster.1.gz to provide /usr/share/man/man1/postmaster.1.gz (postmaster.1.gz) in auto mode\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of start.\n",
            "Setting up postgresql (10+190ubuntu0.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "Processing triggers for systemd (237-3ubuntu10.53) ...\n",
            " * Starting PostgreSQL 10 database server\n",
            "   ...done.\n",
            "ALTER ROLE\n",
            "NOTICE:  database \"reviews\" does not exist, skipping\n",
            "DROP DATABASE\n",
            "CREATE DATABASE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up environment\n",
        "%env TFIO_DEMO_DATABASE_NAME=reviews\n",
        "%env TFIO_DEMO_DATABASE_HOST=localhost\n",
        "%env TFIO_DEMO_DATABASE_PORT=5432\n",
        "%env TFIO_DEMO_DATABASE_USER=postgres\n",
        "%env TFIO_DEMO_DATABASE_PASS=postgres"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz_pVkrtUVx3",
        "outputId": "9094600e-d5e2-4699-d4a2-df8c8582f1d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: TFIO_DEMO_DATABASE_NAME=reviews\n",
            "env: TFIO_DEMO_DATABASE_HOST=localhost\n",
            "env: TFIO_DEMO_DATABASE_PORT=5432\n",
            "env: TFIO_DEMO_DATABASE_USER=postgres\n",
            "env: TFIO_DEMO_DATABASE_PASS=postgres\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *"
      ],
      "metadata": {
        "id": "hfHbDA6VNMyS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I extracted the reviews for Android Applications, which contains 2,638,173 reviews using wget.\n",
        "\n",
        "I then load the reviews into a pyspark dataframe"
      ],
      "metadata": {
        "id": "HOwqW3QIW02K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android.json.gz\"\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"AmazonReviews\") \\\n",
        "    .config(\"spark.driver.extraClassPath\", \"/content/postgresql-42.2.9.jar\")\\\n",
        "    .getOrCreate()\n",
        "df = spark.read.json('/content/reviews_Apps_for_Android.json.gz')"
      ],
      "metadata": {
        "id": "HT5Ic1uRLO7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6ad238e-cc3e-4fcc-a214-c1a94f58c5d9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-12 19:48:51--  http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/reviews_Apps_for_Android.json.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 299986290 (286M) [application/x-gzip]\n",
            "Saving to: ‘reviews_Apps_for_Android.json.gz.1’\n",
            "\n",
            "reviews_Apps_for_An 100%[===================>] 286.09M  40.2MB/s    in 8.4s    \n",
            "\n",
            "2022-05-12 19:49:00 (34.2 MB/s) - ‘reviews_Apps_for_Android.json.gz.1’ saved [299986290/299986290]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "oTransform data from MM DD, YYYY to MM-DD-YYYY by using to_date function.\n",
        "\n",
        "Replace old column with new column containing reformatted dates."
      ],
      "metadata": {
        "id": "TmrF7VPqfN5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.withColumn('newTime', date_format(to_date('reviewTime', format='MM d, yyyy'), \"MM-dd-yyyy\"))\n",
        "df = df.drop(\"reviewTime\")\n",
        "df = df.withColumnRenamed(\"newTime\",\"reviewTime\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "F4HWmtstLPq0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19af86e-6c18-4e04-996f-250318bb66e3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------+-------+--------------------+--------------+--------------------+--------------------+--------------+----------+\n",
            "|      asin| helpful|overall|          reviewText|    reviewerID|        reviewerName|             summary|unixReviewTime|reviewTime|\n",
            "+----------+--------+-------+--------------------+--------------+--------------------+--------------------+--------------+----------+\n",
            "|B004A9SDD8|  [0, 0]|    5.0|Glad to finally s...| AUI0OLXAB3KKT|          A Customer|        Great app!!!|    1301184000|03-27-2011|\n",
            "|B004A9SDD8|[12, 14]|    5.0|this app works gr...|A1ZUSQ3TC3EC4C|           A. Lissak|        Kid loves it|    1321574400|11-18-2011|\n",
            "|B004A9SDD8|  [0, 0]|    4.0|We love these mon...| AC05OAXD72X1V|               Allie| Love these monkeys!|    1367366400|05-01-2013|\n",
            "|B004A9SDD8|  [0, 2]|    5.0|cannot get my kin...|A2RVMFOKBVM21I|     Amazon Customer|fun fun for toddlers|    1350172800|10-14-2012|\n",
            "|B004A9SDD8|  [1, 3]|    1.0|I start this app ...|A3NBSRGUWQGCMZ|     Amazon Customer|Might be great if...|    1300838400|03-23-2011|\n",
            "|B004A9SDD8|  [0, 0]|    5.0|Very engaging to ...|A2KTVCVZJ8GPD2|           A. Mclean|         Great video|    1393545600|02-28-2014|\n",
            "|B004A9SDD8|  [1, 2]|    3.0|My daughter loves...|A2I9RREBHMMPCJ|             AngelaM|great when it worked|    1305504000|05-16-2011|\n",
            "|B004A9SDD8|  [1, 1]|    3.0|Loves the song, s...|A1N4O8VOJZTDVB|      Annette Yancey|         Really cute|    1383350400|11-02-2013|\n",
            "|B004A9SDD8|  [0, 0]|    5.0|Oh, how my little...|A2HQWU6HUKIEC7|Audiobook lover \"...| 2-year-old loves it|    1323043200|12-05-2011|\n",
            "|B004A9SDD8|  [0, 0]|    5.0|I found this at a...|A1SXASF6GYG96I|       Barbara Gibbs|            Fun game|    1337558400|05-21-2012|\n",
            "|B004A9SDD8|  [3, 4]|    5.0|My 1 year old goe...|A2B54P9ZDYH167|Brooke Greenstree...|We love our Monkeys!|    1354752000|12-06-2012|\n",
            "|B004A9SDD8|  [0, 4]|    4.0|Works great on my...|A3Q5R6B7MW8F7Q|              Buck9s|A fun little inte...|    1300924800|03-24-2011|\n",
            "|B004A9SDD8|  [0, 0]|    5.0|This is great for...|A118ECNHF2OCUO|             Carl K.|               great|    1300838400|03-23-2011|\n",
            "|B004A9SDD8|  [0, 0]|    5.0|I haven't found m...|A2BAQGVVFURVDR|                  CB|Awesome game for ...|    1327276800|01-23-2012|\n",
            "|B004A9SDD8|  [1, 1]|    5.0|There are three d...| AFOFZDTX5UC6D|          C. Galindo|This is my grandd...|    1391212800|02-01-2014|\n",
            "|B004A9SDD8|  [0, 1]|    5.0|i downloaded this...|A1MBEM25C3QS5R|               cindy|           dr said??|    1357430400|01-06-2013|\n",
            "|B004A9SDD8|  [1, 1]|    5.0|As a Speech langu...|A2CEK8H02SS23S|           cin_sph66|My patients reque...|    1389916800|01-17-2014|\n",
            "|B004A9SDD8|  [0, 0]|    1.0|Thought it would ...|A1WWW804VWFAHH|          dave hagen|Instead it was ju...|    1404864000|07-09-2014|\n",
            "|B004A9SDD8|  [3, 3]|    5.0|This is absolutel...|A3E6DLEQCZKGDC|             Deborah|           Favorite!|    1377561600|08-27-2013|\n",
            "|B004A9SDD8|  [3, 4]|    1.0|I am so disappoin...| AVYKSESEMLPE3|             Donna S|   Very Disappointed|    1325894400|01-07-2012|\n",
            "+----------+--------+-------+--------------------+--------------+--------------------+--------------------+--------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data into SQL table/server"
      ],
      "metadata": {
        "id": "Zc5zFRwLXDO0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://localhost:5432/reviews\"\n",
        "config = {\"user\":\"postgres\",\n",
        "          \"password\":\"postgres\",  \n",
        "          \"driver\":\"org.postgresql.Driver\"}\n"
      ],
      "metadata": {
        "id": "vw5STubygVNZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.jdbc(url=jdbc_url,\n",
        "              table ='reviews', \n",
        "              mode=mode, properties = config)"
      ],
      "metadata": {
        "id": "UKc_WaYWLQUc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ca3ece6d-87ff-4d90-afc7-5a00050be711"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-5e8e89cf08c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m df.write.jdbc(url=jdbc_url,\n\u001b[1;32m      2\u001b[0m               \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'reviews'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               mode=mode, properties = config)\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjdbc\u001b[0;34m(self, url, table, mode, properties)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0mjprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetProperty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjdbc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjprop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o134.jdbc.\n: java.lang.ClassNotFoundException: org.postgresql.Driver\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.jdbc.DriverRegistry$.register(DriverRegistry.scala:46)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.$anonfun$driverClass$1$adapted(JDBCOptions.scala:101)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.<init>(JDBCOptions.scala:101)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:218)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcOptionsInWrite.<init>(JDBCOptions.scala:222)\n\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:46)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:745)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save data into SQL table/server\n",
        "Save file as parquet file.\n",
        "\n",
        "> Indented block\n",
        "\n"
      ],
      "metadata": {
        "id": "WSNkbzO6XHkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.write.parquet(\"Amazon_reviews.parquet\")"
      ],
      "metadata": {
        "id": "iTpRSGiNLRFN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}